{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "climate_misinfo_detection.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqps8aBfkla0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install allennlp\n",
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiqimiVFk4Ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "# go the working directory\n",
        "os.chdir(\"drive\")\n",
        "os.chdir(\"My Drive\") \n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe7FxjSHddAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.utils.data as Data\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "train_data = []\n",
        "train_labels = []\n",
        "with open('Colab/train_total_balance_new3.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  for v in data.values():\n",
        "    s = '[CLS] ' + v['text'] + ' [SEP]'\n",
        "    train_data.append(s)\n",
        "    train_labels.append(v['label'])\n",
        "f.close()\n",
        "print(\"length_dataset:\",len(train_data))\n",
        "print(train_data[0])\n",
        "\n",
        "dev_data = []\n",
        "dev_labels = []\n",
        "with open('Colab/dev.json') as f:\n",
        "  dev = json.load(f)\n",
        "  for v in dev.values():\n",
        "    s = '[CLS] ' + v['text'] + ' [SEP]'\n",
        "    dev_data.append(s)\n",
        "    dev_labels.append(v['label'])\n",
        "print(\"dev:\",dev_data[0])\n",
        "# print(dev_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovEz7gss3WAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from transformers import BertTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",do_lower_case=True)\n",
        "#train_set\n",
        "tokenized_train=[tokenizer.tokenize(sent) for sent in train_data] \n",
        "#dev_set\n",
        "tokenized_dev=[tokenizer.tokenize(sent) for sent in dev_data] \n",
        "print(\"tokenized_dev:\",tokenized_dev[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pzY-uWWWbOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a function for pad_sequences from both pre and post \n",
        "def truncating_from_middle(input_lists,maxlen,value=0):\n",
        "  half = int(maxlen/2)\n",
        "  head = 128\n",
        "  tail = 384\n",
        "  new_lists = []\n",
        "  for l in input_lists:\n",
        "    if len(l) > maxlen:\n",
        "      post = (len(l)-tail)\n",
        "      new_l = l[:head] + l[post:]\n",
        "      new_lists.append(new_l)\n",
        "    else:\n",
        "      pad_need = maxlen-len(l)\n",
        "      l = l + [0] * pad_need\n",
        "      new_lists.append(l)\n",
        "  return new_lists"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZqh5jHrQgBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 256\n",
        "#convert to ids format\n",
        "input_ids=[tokenizer.convert_tokens_to_ids(sent) for sent in tokenized_train]\n",
        "dev_input_ids=[tokenizer.convert_tokens_to_ids(sent) for sent in tokenized_dev]\n",
        "\n",
        "#padding sent to fixed length\n",
        "# input_ids = truncating_from_middle(input_ids,maxlen=MAX_LEN,value=0)\n",
        "# dev_input_ids = truncating_from_middle(dev_input_ids,maxlen=MAX_LEN,value=0)\n",
        "input_ids=pad_sequences(input_ids, value=0, maxlen=MAX_LEN, dtype=\"long\", truncating=\"pre\", padding=\"post\")\n",
        "dev_input_ids=pad_sequences(dev_input_ids, value=0, maxlen=MAX_LEN, dtype=\"long\", truncating=\"pre\", padding=\"post\")\n",
        "print(\"input_ids:\",len(input_ids[0]))\n",
        "#build attention mask\n",
        "train_masks = []\n",
        "for seq in input_ids:\n",
        "  seq_mask = [int(i>0) for i in seq]\n",
        "  train_masks.append(seq_mask)\n",
        "\n",
        "validation_masks = []\n",
        "for seq in dev_input_ids:\n",
        "  seq_mask = [int(i>0) for i in seq]\n",
        "  validation_masks.append(seq_mask)\n",
        "\n",
        "train_inputs = input_ids\n",
        "validation_inputs = dev_input_ids\n",
        "validation_labels = dev_labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BN66Rng8xMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#transfer dataset to tensor\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "#create dataloader\n",
        "batch_size = 32\n",
        "train_data = Data.TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = Data.RandomSampler(train_data)\n",
        "train_dataloader = Data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "validation_data = Data.TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = Data.SequentialSampler(validation_data)\n",
        "validation_dataloader = Data.DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "print(\"Create dataloader done!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWWY1_CL9DIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "# load pre trained model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aDR15IgAE_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "# set up optimizer, epochs\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr=2e-5,\n",
        "                  eps=1e-5)\n",
        "epochs = 4\n",
        "\n",
        "# learning rate scheduler\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0, #default\n",
        "                                            num_training_steps=total_steps) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhCZLQO9H46a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import trange\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# function to calculate accuracy\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# start train\n",
        "train_loss_set = [] # store loss for plotting after training\n",
        "for epoch in trange(epochs, desc=\"Epoch\"):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        #BertForSequenceClassification [0]Lossï¼Œ[1]logits\n",
        "        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)[0]\n",
        "        train_loss_set.append(loss.item())\n",
        "        \n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "    print(\"Train loss: {}\".format(total_loss / nb_tr_steps))\n",
        "    \n",
        "    # save models\n",
        "    PATH = 'Colab/model_epoch_' + str(epoch) +'.pth'\n",
        "    torch.save(model.state_dict(), PATH)\n",
        "    # evaluation\n",
        "    print(\"Running evaluation...\")\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    dev_predicts = []\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "          logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        preds = np.argmax(logits, axis=1)\n",
        "        dev_predicts = np.concatenate((dev_predicts,preds))\n",
        "        \n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        nb_eval_steps += 1\n",
        "    p, r, f, _ = precision_recall_fscore_support(dev_labels, dev_predicts, pos_label=1, average=\"binary\")\n",
        "    print(\"scoring.py:precision:\",p,\" recall:\",r,\" f1_score:\",f)\n",
        "    print(\"Validation Accuracy: {}\".format(eval_accuracy / nb_eval_steps))\n",
        "print(\"\\nTraining complete!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDZNEtPOfyNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = []\n",
        "test_labels = []\n",
        "with open('Colab/test-unlabelled.json','r') as f:\n",
        "  data = json.load(f)\n",
        "  for v in data.values():\n",
        "    s = '[CLS] ' + v['text'] + ' [SEP]'\n",
        "    test_data.append(s)\n",
        "    test_labels.append(1)\n",
        "f.close()\n",
        "print(\"length_dataset:\",len(test_data))\n",
        "print(test_data[0])\n",
        "print(test_labels[0])\n",
        "\n",
        "tokenized_test=[tokenizer.tokenize(sent) for sent in test_data] \n",
        "test_input_ids=[tokenizer.convert_tokens_to_ids(sent) for sent in tokenized_test]\n",
        "# test_input_ids = truncating_from_middle(test_input_ids,maxlen=MAX_LEN,value=0)\n",
        "test_input_ids=pad_sequences(test_input_ids, value=0, maxlen=MAX_LEN, dtype=\"long\", truncating=\"pre\", padding=\"post\")\n",
        "\n",
        "print(\"generate masks\")\n",
        "test_masks = []\n",
        "for seq in test_input_ids:\n",
        "  seq_mask = [int(i>0) for i in seq]\n",
        "  test_masks.append(seq_mask)\n",
        "\n",
        "test_inputs = test_input_ids\n",
        "# test_labels\n",
        "# test_masks\n",
        "\n",
        "print(\"transfer to tensor\")\n",
        "test_inputs = torch.tensor(test_inputs)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "test_masks = torch.tensor(test_masks)\n",
        "\n",
        "#create dataloader\n",
        "test_data = Data.TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = Data.SequentialSampler(test_data)\n",
        "test_dataloader = Data.DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEV-QbZFpqNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "model.load_state_dict(torch.load('Colab/model_epoch_2.pth'))\n",
        "model.eval()\n",
        "test_predicts = []\n",
        "for batch in test_dataloader:\n",
        "# for batch in validation_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  with torch.no_grad():\n",
        "    outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask)\n",
        "  logits = outputs[0]\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  # label_ids = b_labels.to('cpu').numpy()\n",
        "  preds = np.argmax(logits, axis=1)\n",
        "  test_predicts = np.concatenate((test_predicts,preds))\n",
        "  \n",
        "  # print(preds)\n",
        "\n",
        "print(test_predicts)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ov4PiqD64vVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"store now...\")\n",
        "test_predicts = [int(i) for i in test_predicts]\n",
        "print(test_predicts)\n",
        "state = {}\n",
        "counttt = 0\n",
        "u_r = [3, 5, 6, 8, 13, 17, 20, 22, 23, 24, 28, 30, 32, 35, 36, 39, 45, 46, 47, 49, 50, 53, 54, 55, 60, 62, 64, 65, 66, 68, 69, 73, 74, 75, 78, 79, 80, 81, 82, 88, 89, 91, 93, 94, 95, 96, 97, 98, 100, 101, 102, 103, 106, 111, 114, 119, 120, 121, 122, 123, 131, 137, 138, 140, 141, 144, 145, 147, 149, 151, 152, 154, 159, 161, 163, 165, 167, 168, 170, 172, 175, 176, 178, 184, 187, 191, 193, 194, 196, 199, 200, 202, 204, 206, 209, 210, 211, 213, 216, 218, 220, 221, 225, 227, 228, 229, 230, 232, 235, 238, 239, 240, 241, 243, 245, 246, 250, 251, 252, 254, 258, 259, 262, 263, 264, 268, 271, 272, 276, 278, 279, 280, 284, 286, 288, 292, 297, 298, 299, 300, 301, 305, 310, 312, 313, 314, 315, 316, 317, 321, 322, 323, 325, 326, 327, 328, 331, 332, 333, 334, 335, 336, 338, 339, 343, 346, 349, 350, 351, 353, 354, 355, 360, 362, 365, 367, 369, 371, 373, 374, 375, 376, 378, 382, 384, 385, 393, 394, 395, 396, 398, 405, 412, 413, 416, 418, 420, 422, 423, 424, 425, 428, 429, 430, 431, 433, 435, 437, 438, 439, 440, 441, 442, 443, 444, 446, 448, 453, 454, 457, 459, 460, 461, 462, 466, 467, 468, 470, 471, 473, 474, 481, 482, 486, 489, 490, 493, 497, 500, 503, 504, 505, 507, 512, 513, 514, 515, 519, 522, 523, 528, 529, 530, 531, 532, 533, 534, 536, 537, 541, 548, 549, 551, 559, 562, 563, 565, 573, 574, 577, 585, 587, 588, 590, 597, 599, 600, 603, 606, 607, 609, 611, 613, 614, 618, 619, 621, 628, 635, 641, 646, 647, 649, 651, 652, 654, 656, 661, 662, 668, 669, 671, 674, 675, 676, 679, 680, 683, 685, 686, 689, 690, 694, 696, 697, 698, 699, 701, 702, 704, 705, 709, 710, 711, 712, 716, 718, 721, 725, 727, 728, 732, 733, 734, 736, 737, 739, 741, 744, 746, 747, 748, 751, 753, 754, 756, 757, 759, 761, 762, 766, 767, 774, 787, 788, 789, 790, 793, 794, 795, 796, 797, 803, 804, 812, 814, 818, 821, 823, 827, 829, 830, 832, 834, 836, 842, 843, 844, 847, 848, 849, 851, 853, 854, 855, 856, 858, 860, 861, 863, 864, 865, 866, 870, 871, 872, 878, 880, 883, 884, 889, 892, 893, 894, 898, 899, 901, 907, 909, 910, 912, 914, 915, 917, 918, 920, 921, 922, 923, 924, 925, 926, 930, 938, 945, 947, 949, 951, 952, 953, 955, 956, 957, 960, 962, 964, 966, 968, 969, 971, 972, 973, 975, 978, 980, 985, 986, 988, 989, 992, 997, 999, 1000, 1004, 1006, 1007, 1009, 1010, 1011, 1014, 1018, 1019, 1021, 1022, 1023, 1024, 1026, 1030, 1033, 1034, 1036, 1037, 1039, 1042, 1043, 1044, 1045, 1048, 1051, 1053, 1054, 1055, 1056, 1057, 1058, 1062, 1065, 1070, 1072, 1073, 1074, 1075, 1081, 1083, 1084, 1085, 1089, 1090, 1091, 1094, 1097, 1098, 1099, 1103, 1104, 1106, 1113, 1115, 1116, 1117, 1118, 1119, 1120, 1124, 1125, 1132, 1136, 1137, 1138, 1139, 1142, 1143, 1148, 1151, 1152, 1154, 1156, 1157, 1160, 1161, 1165, 1167, 1168, 1170, 1172, 1175, 1176, 1178, 1179, 1181, 1182, 1184, 1186, 1190, 1192, 1193, 1198, 1199, 1201, 1202, 1203, 1205, 1216, 1218, 1221, 1222, 1224, 1225, 1228, 1230, 1236, 1237, 1240, 1241, 1242, 1244, 1245, 1247, 1256, 1258, 1260, 1261, 1262, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1278, 1279, 1280, 1282, 1283, 1284, 1285, 1286, 1288, 1292, 1299, 1301, 1302, 1303, 1304, 1307, 1313, 1315, 1316, 1317, 1318, 1320, 1321, 1325, 1326, 1328, 1329, 1330, 1331, 1332, 1335, 1338, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1352, 1355, 1356, 1358, 1359, 1361, 1363, 1365, 1366, 1368, 1370, 1373, 1375, 1378, 1379, 1381, 1382, 1384, 1390, 1397, 1398, 1403, 1406]\n",
        "for i in u_r:\n",
        "    test_predicts[i] = 0\n",
        "\n",
        "for label in test_predicts:\n",
        "  key = \"test-\" + str(counttt)\n",
        "  a = {\"label\":label}\n",
        "  state[key] = a\n",
        "  counttt += 1\n",
        "\n",
        "with open('Colab/test-output1.json','w') as f:\n",
        "  json.dump(state,f)\n",
        "  f.close()\n",
        "\n",
        "print(\"finish all tasks in prediction!!!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLZCFrJe-Nja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi\n",
        "# check gpu usage\n",
        "# check gpu use for a particular process\n",
        "# !pmap -d 125\n",
        "# !sudo apt-get install psmisc\n",
        "# !fuser -v /dev/nvidia*\n",
        "# !kill -9 542"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}