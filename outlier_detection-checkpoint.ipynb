{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import re\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = TreebankWordTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json','r') as f:\n",
    "    text = f.read()\n",
    "    text = json.loads(text)\n",
    "\n",
    "f.close()\n",
    "# print(text['train-0'])\n",
    "\n",
    "train_set = []\n",
    "for i,t in text.items():\n",
    "    train_set.append(t['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dev.json','r') as f:\n",
    "    dev = f.read()\n",
    "    dev = json.loads(dev)\n",
    "f.close()\n",
    "\n",
    "dev_set = []\n",
    "dev_label = []\n",
    "for i,t in dev.items():\n",
    "    dev_set.append(t['text'])\n",
    "    dev_label.append(t['label'])\n",
    "\n",
    "with open('test-unlabelled.json','r') as f:\n",
    "    test = f.read()\n",
    "    test = json.loads(test)\n",
    "    \n",
    "test_set = []\n",
    "for i,t in test.items():\n",
    "    test_set.append(t['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j = 85\n",
    "# print(dev_set[j])\n",
    "# print('----------')\n",
    "# print(dev_label[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_set = [re.sub(r'http\\S+', '', s) for s in train_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1168x21179 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 233880 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('\\w+|\\w+-\\w+')\n",
    "\n",
    "def my_tokenize(s):\n",
    "#     tokenizer = TreebankWordTokenizer()\n",
    "#     tokens = nltk.word_tokenize(s)\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in tokens]\n",
    "    return lemma_words\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=my_tokenize,lowercase=True,stop_words='english',ngram_range=(1,1)) \n",
    "x_train = vectorizer.fit_transform(processed_train_set)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vectorizer.vocabulary_)          \n",
    "# print(vectorizer.get_feature_names())\n",
    "#print(x_train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1168, 21179)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "x_train_tfidf = tfidf_transformer.fit_transform(x_train)\n",
    "x_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)\n",
    "# SVD_x_train = svd.fit_transform(x_train_tfidf)\n",
    "# svd.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd.explained_variance_ratio_\n",
    "# SVD_x_train.shape\n",
    "# feature_path = 'models/feature.pkl'\n",
    "# with open(feature_path, 'wb') as fw:\n",
    "#     pickle.dump(vectorizer.vocabulary_, fw)\n",
    "#     tfidftransformer_path = 'models/tfidftransformer.pkl'\n",
    "# with open(tfidftransformer_path, 'wb') as fw:\n",
    "#     pickle.dump(tfidftransformer, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lizhengyang/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 21179)\n"
     ]
    }
   ],
   "source": [
    "# generate dev set\n",
    "processed_dev_set = [re.sub(r'http\\S+', '', s) for s in dev_set]\n",
    "vectorizer_dev = CountVectorizer(tokenizer=my_tokenize,lowercase=True,stop_words='english',\n",
    "                                 vocabulary=vectorizer.vocabulary_)\n",
    "x_dev = vectorizer_dev.fit_transform(processed_dev_set)\n",
    "x_dev_tfidf = tfidf_transformer.fit_transform(x_dev)\n",
    "print(x_dev_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_f1: 0.6881720430107526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# grid = {'gamma' :[1e-6,1e-5,0.0001,0.001,0.01,0.1],\n",
    "#         'nu' : np.linspace(0.40, 0.80, 40)}\n",
    "grid = {'gamma' : [0.0001],\n",
    "        'nu' : [0.71]}\n",
    "clf = svm.OneClassSVM(kernel=\"rbf\")\n",
    "\n",
    "result_dict = {}\n",
    "count = 0\n",
    "f1_max = []\n",
    "for z in tqdm(ParameterGrid(grid)):\n",
    "    clf.set_params(**z)\n",
    "    clf.fit(x_train_tfidf)\n",
    "    result = clf.predict(x_dev_tfidf)\n",
    "    result = [i if i==1 else 0 for i in result]\n",
    "    p, r, f, _ = precision_recall_fscore_support(dev_label, result, pos_label=1, average=\"binary\")\n",
    "#     acc = np.mean(result == np.array(dev_label))\n",
    "    keys = str(count)\n",
    "    count+=1\n",
    "    result_dict[keys] = {\"para\":z,\"p\":p,\"r\":r,\"f1\":f}\n",
    "    \n",
    "    f1_max.append(f)\n",
    "    \n",
    "max_f = np.max(f1_max)\n",
    "print(\"max_f1:\",max_f)\n",
    "a = f1_max.index(max_f)\n",
    "# print(result_dict[a])\n",
    "# print(result_dict)    \n",
    "    \n",
    "# clf.fit(x_train_tfidf)\n",
    "# clf.fit(SVD_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'para': {'gamma': 0.0001, 'nu': 0.71}, 'p': 0.7441860465116279, 'r': 0.64, 'f1': 0.6881720430107526}}\n"
     ]
    }
   ],
   "source": [
    "# a = f1_max.index(0.7538461538461539)\n",
    "# print(result_dict['group140'])\n",
    "print(result_dict)\n",
    "\n",
    "\n",
    "\n",
    "# result_dict['group565']\n",
    "\n",
    "\n",
    "#{'para': {'gamma': 0.0001, 'nu': 0.7100000000000001}, 'acc': 0.71} # better\n",
    "#{'para': {'gamma': 'auto', 'nu': 0.71}, 'acc': 0.7}}\n",
    "# {'para': {'gamma': 1e-06, 'nu': 0.6051282051282052}, 'p': 0.6125, 'r': 0.98, 'f1': 0.7538461538461539}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD_x_dev = svd.fit_transform(x_dev_tfidf)\n",
    "# result = clf.predict(x_dev_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "# write result into json file\n",
    "result_dict = {}\n",
    "for i in range(len(result)):\n",
    "    key = \"dev-{}\".format(i)\n",
    "    result_dict[key] = {\"label\":int(result[i])}\n",
    "print(type(result_dict))\n",
    "import pickle\n",
    "with open(\"dev-predict.json\",\"w\") as f:\n",
    "    json.dump(result_dict,f)\n",
    "    print(\"finish\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1410, 21179)\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "# predict on test and give an output\n",
    "processed_test_set = [re.sub(r'http\\S+', '', s) for s in test_set]\n",
    "vectorizer_test = CountVectorizer(tokenizer=my_tokenize,lowercase=True,stop_words='english',\n",
    "                                 vocabulary=vectorizer.vocabulary_)\n",
    "x_test = vectorizer_test.fit_transform(processed_test_set)\n",
    "# x_dev.shape\n",
    "x_test_tfidf = tfidf_transformer.fit_transform(x_test)\n",
    "print(x_test_tfidf.shape)\n",
    "# SVD_x_dev = svd.fit_transform(x_dev_tfidf)\n",
    "result_test = clf.predict(x_test_tfidf)\n",
    "result_test = [i if i==1 else 0 for i in result_test]\n",
    "\n",
    "result_dict_test = {}\n",
    "for i in range(len(result_test)):\n",
    "    key = \"test-{}\".format(i)\n",
    "    result_dict_test[key] = {\"label\":int(result_test[i])}\n",
    "\n",
    "with open(\"test-output.json\",\"w\") as f:\n",
    "    json.dump(result_dict_test,f)\n",
    "    print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
