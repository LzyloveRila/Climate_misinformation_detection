{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import re\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register preprocessing class\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data set(train,dev,test)\n",
    "with open('train.json','r') as f:\n",
    "    text = f.read()\n",
    "    text = json.loads(text)\n",
    "f.close()\n",
    "\n",
    "train_set = []\n",
    "for i,t in text.items():\n",
    "    train_set.append(t['text'])\n",
    "    \n",
    "with open('dev.json','r') as f:\n",
    "    dev = f.read()\n",
    "    dev = json.loads(dev)\n",
    "f.close()\n",
    "\n",
    "dev_set = []\n",
    "dev_label = []\n",
    "for i,t in dev.items():\n",
    "    dev_set.append(t['text'])\n",
    "    dev_label.append(t['label'])\n",
    "\n",
    "with open('test-unlabelled.json','r') as f:\n",
    "    test = f.read()\n",
    "    test = json.loads(test)\n",
    "    \n",
    "test_set = []\n",
    "for i,t in test.items():\n",
    "    test_set.append(t['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1168x23859 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 242075 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('\\w+|\\w+-\\w+')\n",
    "\n",
    "def my_tokenize(s):\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in tokens]\n",
    "    return tokens\n",
    "\n",
    "processed_train_set = [re.sub(r'http\\S+', '', s) for s in train_set]\n",
    "vectorizer = CountVectorizer(tokenizer=my_tokenize,lowercase=True,stop_words='english',ngram_range=(1,1)) \n",
    "x_train = vectorizer.fit_transform(processed_train_set)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1168, 23859)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "x_train_tfidf = tfidf_transformer.fit_transform(x_train)\n",
    "x_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 23859)\n"
     ]
    }
   ],
   "source": [
    "# generate dev set\n",
    "processed_dev_set = [re.sub(r'http\\S+', '', s) for s in dev_set]\n",
    "vectorizer_dev = CountVectorizer(tokenizer=my_tokenize,lowercase=True,stop_words='english',\n",
    "                                 vocabulary=vectorizer.vocabulary_)\n",
    "x_dev = vectorizer_dev.fit_transform(processed_dev_set)\n",
    "x_dev_tfidf = tfidf_transformer.fit_transform(x_dev)\n",
    "print(x_dev_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_f1: 0.7401574803149605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# grid search\n",
    "# grid = {'gamma' :[1e-6,1e-5,0.0001,0.001,0.01,0.1],\n",
    "#         'nu' : np.linspace(0.40, 0.80, 40)}\n",
    "grid = {'gamma' : [0.0001],\n",
    "        'nu' : [0.47]}\n",
    "clf = svm.OneClassSVM(kernel=\"rbf\")\n",
    "#kernel：核函数（一般使用高斯核）\n",
    "\n",
    "　　#nu：设定训练误差(0, 1]，表示异常点比例，默认值为0.5\n",
    "    # gamma 惩罚项\n",
    "result_dict = {}\n",
    "count = 0\n",
    "f1_max = []\n",
    "for z in tqdm(ParameterGrid(grid)):\n",
    "    clf.set_params(**z)\n",
    "    clf.fit(x_train_tfidf)\n",
    "    result = clf.predict(x_dev_tfidf)\n",
    "    result = [i if i==1 else 0 for i in result]\n",
    "    p, r, f, _ = precision_recall_fscore_support(dev_label, result, pos_label=1, average=\"binary\")\n",
    "    keys = str(count)\n",
    "    count+=1\n",
    "    result_dict[keys] = {\"para\":z,\"p\":p,\"r\":r,\"f1\":f}\n",
    "    \n",
    "    f1_max.append(f)\n",
    "    \n",
    "max_f = np.max(f1_max)\n",
    "print(\"max_f1:\",max_f)\n",
    "a = f1_max.index(max_f)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "# write result into json file\n",
    "result_dict = {}\n",
    "for i in range(len(result)):\n",
    "    key = \"dev-{}\".format(i)\n",
    "    result_dict[key] = {\"label\":int(result[i])}\n",
    "\n",
    "import pickle\n",
    "with open(\"dev-predict.json\",\"w\") as f:\n",
    "    json.dump(result_dict,f)\n",
    "    print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1410, 21179)\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "# predict on test and give an output\n",
    "processed_test_set = [re.sub(r'http\\S+', '', s) for s in test_set]\n",
    "vectorizer_test = CountVectorizer(tokenizer=my_tokenize,lowercase=True,stop_words='english',\n",
    "                                 vocabulary=vectorizer.vocabulary_)\n",
    "x_test = vectorizer_test.fit_transform(processed_test_set)\n",
    "x_test_tfidf = tfidf_transformer.fit_transform(x_test)\n",
    "print(x_test_tfidf.shape)\n",
    "\n",
    "result_test = clf.predict(x_test_tfidf)\n",
    "result_test = [i if i==1 else 0 for i in result_test]\n",
    "\n",
    "result_dict_test = {}\n",
    "for i in range(len(result_test)):\n",
    "    key = \"test-{}\".format(i)\n",
    "    result_dict_test[key] = {\"label\":int(result_test[i])}\n",
    "\n",
    "with open(\"test-output.json\",\"w\") as f:\n",
    "    json.dump(result_dict_test,f)\n",
    "    print(\"finish\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
