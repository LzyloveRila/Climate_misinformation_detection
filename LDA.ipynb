{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open('test-unlabelled.json','r') as f:\n",
    "    text = f.read()\n",
    "    text = json.loads(text)\n",
    "f.close()\n",
    "\n",
    "test_data = []\n",
    "for i,t in text.items():\n",
    "    test_data.append(t['text'])\n",
    "    \n",
    "with open('train.json','r') as f:\n",
    "    text1 = f.read()\n",
    "    text1 = json.loads(text1)\n",
    "f.close()\n",
    "\n",
    "train_data = []\n",
    "for i,t in text1.items():\n",
    "    train_data.append(t['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing function\n",
    "def preprocessing(text):\n",
    "    text = text.replace('{html}',\"\")\n",
    "    rem_url = re.sub(r'http\\S+', '', text)\n",
    "    tokens = nltk.word_tokenize(rem_url)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    rem_stop_words  = [w for w in tokens if len(w)>2 if not w in stopwords.words('english')]\n",
    "    pos_tag = nltk.pos_tag(rem_stop_words)\n",
    "    remain_NN = [w for w,pos in pos_tag if pos.startswith('NN')]\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in remain_NN]\n",
    "    \n",
    "    return \" \".join(lemma_words)\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    # print the term with higher weight in the topic\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print(model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [preprocessing(text) for text in test_data]\n",
    "train_set = [preprocessing(text) for text in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1410\n",
      "Topic #0:\n",
      "climate change emission year government energy world carbon country gas people policy fuel action power coal plan report target minister\n",
      "Topic #1:\n",
      "year life time child day school music home baby story family son student way car thing week parent church mother\n",
      "Topic #2:\n",
      "president state government country war force people report group leader year nation official security law obama time party right isi\n",
      "Topic #3:\n",
      "climate change temperature year scientist model data science level sea ice time study co2 water world record warming earth weather\n",
      "Topic #4:\n",
      "hotel time home year house work space room city beach people property art view world image artist day building area\n",
      "Topic #5:\n",
      "people police city man attack group street resident protester officer station area video official news protest security government men day\n",
      "Topic #6:\n",
      "people year time child health company day film woman family service month thing user way case food week cancer state\n",
      "Topic #7:\n",
      "plane flight passenger aircraft water hour area day airline airport time air week road ship foot mile authority home hospital\n",
      "Topic #8:\n",
      "court year police family school death woman case child officer judge time student home charge wife attorney report state county\n",
      "Topic #9:\n",
      "game player world time team year season club cup football city fan goal sport league play match champion manager day\n",
      "\n",
      "[[0.10005452 2.09965701 0.1000088  ... 0.10000207 0.10000513 0.10000551]\n",
      " [0.10002609 0.1        5.09993316 ... 0.1        0.1000096  0.10002226]\n",
      " [0.10001971 0.10011907 1.09989822 ... 0.1        4.09990671 0.10000025]\n",
      " ...\n",
      " [0.10002917 0.1000047  0.10001161 ... 0.10004744 0.1000023  0.1       ]\n",
      " [9.30825655 0.10000154 0.10000018 ... 0.10000767 0.10000658 0.10002733]\n",
      " [7.79373358 0.10000338 0.10000748 ... 0.1000008  0.10000789 3.29362868]]\n"
     ]
    }
   ],
   "source": [
    "# transfer to BOW format\n",
    "tf_vectorizer = CountVectorizer(max_df=0.9, min_df=2,stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(test_set)\n",
    "vectorizer_test = CountVectorizer(max_df=0.9, min_df=2,stop_words='english',vocabulary=tf_vectorizer.vocabulary_)\n",
    "tf_positive = vectorizer_test.fit_transform(train_set) \n",
    "\n",
    "# model fitting\n",
    "n_topic = 10\n",
    "lda = LatentDirichletAllocation(n_components=n_topic, max_iter=1000,learning_method='batch')\n",
    "lda.fit(tf)        \n",
    "\n",
    "n_top_words=20\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "# visual keywords in 10 topics\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2114.732864240092"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.perplexity(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[575, 6, 49, 498, 6, 0, 8, 1, 20, 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "article_topic = [0] * 10\n",
    "# perdiction\n",
    "for doc in tf_positive:\n",
    "    doc_scores = lda.transform(doc)\n",
    "    topic = np.argmax(doc_scores)\n",
    "    article_topic[topic] += 1\n",
    "print(article_topic)# 0,2,3,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dev.json','r') as f:\n",
    "    text = f.read()\n",
    "    text = json.loads(text)\n",
    "f.close()\n",
    "\n",
    "dev_data = []\n",
    "dev_label = []\n",
    "for i,t in text.items():\n",
    "    dev_data.append(t['text'])\n",
    "    dev_label.append(t['label'])\n",
    "    \n",
    "dev_set = [preprocessing(text) for text in dev_data]\n",
    "vectorizer_dev = CountVectorizer(max_df=0.9, min_df=2,stop_words='english',vocabulary=tf_vectorizer.vocabulary_)\n",
    "tf_dev = vectorizer_test.fit_transform(dev_set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[235, 85, 223, 107, 68, 80, 183, 72, 201, 156]\n"
     ]
    }
   ],
   "source": [
    "article_topic = [0] * 10\n",
    "unrelated_news = []\n",
    "for i,doc in enumerate(tf):\n",
    "    doc_scores = lda.transform(doc)\n",
    "    topic = np.argmax(doc_scores)\n",
    "    article_topic[topic] += 1\n",
    "    if topic in (1,4,5,6,7,9):\n",
    "        unrelated_news.append(i)\n",
    "print(article_topic)# 0,2,3,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 5, 6, 8, 13, 17, 20, 22, 23, 24, 28, 30, 32, 35, 36, 39, 45, 46, 47, 49, 50, 53, 54, 55, 60, 62, 64, 65, 66, 68, 69, 73, 74, 75, 78, 79, 80, 81, 82, 88, 89, 91, 93, 94, 95, 96, 97, 98, 100, 101, 102, 103, 106, 111, 114, 119, 120, 121, 122, 123, 131, 137, 138, 140, 141, 144, 145, 147, 149, 151, 152, 154, 159, 161, 163, 165, 167, 168, 170, 172, 175, 176, 178, 184, 187, 191, 193, 194, 196, 199, 200, 202, 204, 206, 209, 210, 211, 213, 216, 218, 220, 221, 225, 227, 228, 229, 230, 232, 235, 238, 239, 240, 241, 243, 245, 246, 250, 251, 252, 254, 258, 259, 262, 263, 264, 268, 271, 272, 276, 278, 279, 280, 284, 286, 288, 292, 297, 298, 299, 300, 301, 305, 310, 312, 313, 314, 315, 316, 317, 321, 322, 323, 325, 326, 327, 328, 331, 332, 333, 334, 335, 336, 338, 339, 343, 346, 349, 350, 351, 353, 354, 355, 360, 362, 365, 367, 369, 371, 373, 374, 375, 376, 378, 382, 384, 385, 393, 394, 395, 396, 398, 405, 412, 413, 416, 418, 420, 422, 423, 424, 425, 428, 429, 430, 431, 433, 435, 437, 438, 439, 440, 441, 442, 443, 444, 446, 448, 453, 454, 457, 459, 460, 461, 462, 466, 467, 468, 470, 471, 473, 474, 481, 482, 486, 489, 490, 493, 497, 500, 503, 504, 505, 507, 512, 513, 514, 515, 519, 522, 523, 528, 529, 530, 531, 532, 533, 534, 536, 537, 541, 548, 549, 551, 559, 562, 563, 565, 573, 574, 577, 585, 587, 588, 590, 597, 599, 600, 603, 606, 607, 609, 611, 613, 614, 618, 619, 621, 628, 635, 641, 646, 647, 649, 651, 652, 654, 656, 661, 662, 668, 669, 671, 674, 675, 676, 679, 680, 683, 685, 686, 689, 690, 694, 696, 697, 698, 699, 701, 702, 704, 705, 709, 710, 711, 712, 716, 718, 721, 725, 727, 728, 732, 733, 734, 736, 737, 739, 741, 744, 746, 747, 748, 751, 753, 754, 756, 757, 759, 761, 762, 766, 767, 774, 787, 788, 789, 790, 793, 794, 795, 796, 797, 803, 804, 812, 814, 818, 821, 823, 827, 829, 830, 832, 834, 836, 842, 843, 844, 847, 848, 849, 851, 853, 854, 855, 856, 858, 860, 861, 863, 864, 865, 866, 870, 871, 872, 878, 880, 883, 884, 889, 892, 893, 894, 898, 899, 901, 907, 909, 910, 912, 914, 915, 917, 918, 920, 921, 922, 923, 924, 925, 926, 930, 938, 945, 947, 949, 951, 952, 953, 955, 956, 957, 960, 962, 964, 966, 968, 969, 971, 972, 973, 975, 978, 980, 985, 986, 988, 989, 992, 997, 999, 1000, 1004, 1006, 1007, 1009, 1010, 1011, 1014, 1018, 1019, 1021, 1022, 1023, 1024, 1026, 1030, 1033, 1034, 1036, 1037, 1039, 1042, 1043, 1044, 1045, 1048, 1051, 1053, 1054, 1055, 1056, 1057, 1058, 1062, 1065, 1070, 1072, 1073, 1074, 1075, 1081, 1083, 1084, 1085, 1089, 1090, 1091, 1094, 1097, 1098, 1099, 1103, 1104, 1106, 1113, 1115, 1116, 1117, 1118, 1119, 1120, 1124, 1125, 1132, 1136, 1137, 1138, 1139, 1142, 1143, 1148, 1151, 1152, 1154, 1156, 1157, 1160, 1161, 1165, 1167, 1168, 1170, 1172, 1175, 1176, 1178, 1179, 1181, 1182, 1184, 1186, 1190, 1192, 1193, 1198, 1199, 1201, 1202, 1203, 1205, 1216, 1218, 1221, 1222, 1224, 1225, 1228, 1230, 1236, 1237, 1240, 1241, 1242, 1244, 1245, 1247, 1256, 1258, 1260, 1261, 1262, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1278, 1279, 1280, 1282, 1283, 1284, 1285, 1286, 1288, 1292, 1299, 1301, 1302, 1303, 1304, 1307, 1313, 1315, 1316, 1317, 1318, 1320, 1321, 1325, 1326, 1328, 1329, 1330, 1331, 1332, 1335, 1338, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1352, 1355, 1356, 1358, 1359, 1361, 1363, 1365, 1366, 1368, 1370, 1373, 1375, 1378, 1379, 1381, 1382, 1384, 1390, 1397, 1398, 1403, 1406]\n"
     ]
    }
   ],
   "source": [
    "print(unrelated_news)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
